{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4078596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Dense, Concatenate, Dropout, \n",
    "    BatchNormalization, Flatten\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "class CTRDeepNN:\n",
    "    def __init__(self, categorical_features_info, numerical_features_count):\n",
    "        \"\"\"\n",
    "        Initialize CTR Deep Neural Network\n",
    "        \n",
    "        Args:\n",
    "            categorical_features_info: dict with feature_name: vocab_size\n",
    "            numerical_features_count: int, number of numerical features\n",
    "        \"\"\"\n",
    "        self.categorical_features_info = categorical_features_info\n",
    "        self.numerical_features_count = numerical_features_count\n",
    "        self.model = None\n",
    "        \n",
    "    def build_model(self, embedding_dim=50, hidden_layers=[512, 256, 128], \n",
    "                   dropout_rate=0.3, l2_reg=1e-5):\n",
    "        \"\"\"\n",
    "        Build the deep neural network model\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: dimension for embeddings\n",
    "            hidden_layers: list of hidden layer sizes\n",
    "            dropout_rate: dropout rate for regularization\n",
    "            l2_reg: L2 regularization factor\n",
    "        \"\"\"\n",
    "        \n",
    "        # Input layers\n",
    "        inputs = []\n",
    "        embeddings = []\n",
    "        \n",
    "        # 1. Create embedding layers for categorical features\n",
    "        for feature_name, vocab_size in self.categorical_features_info.items():\n",
    "            input_layer = Input(shape=(1,), name=f'input_{feature_name}')\n",
    "            inputs.append(input_layer)\n",
    "            \n",
    "            # Calculate embedding dimension (rule of thumb: min(50, vocab_size//2))\n",
    "            emb_dim = min(embedding_dim, max(vocab_size // 2, 1))\n",
    "            \n",
    "            embedding_layer = Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=emb_dim,\n",
    "                embeddings_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                name=f'embedding_{feature_name}'\n",
    "            )(input_layer)\n",
    "            \n",
    "            # Flatten the embedding\n",
    "            embedding_flat = Flatten()(embedding_layer)\n",
    "            embeddings.append(embedding_flat)\n",
    "        \n",
    "        # 2. Input layer for numerical features\n",
    "        if self.numerical_features_count > 0:\n",
    "            numerical_input = Input(shape=(self.numerical_features_count,), \n",
    "                                  name='numerical_features')\n",
    "            inputs.append(numerical_input)\n",
    "            \n",
    "            # Optional: apply batch normalization to numerical features\n",
    "            numerical_bn = BatchNormalization(name='numerical_bn')(numerical_input)\n",
    "            embeddings.append(numerical_bn)\n",
    "        \n",
    "        # 3. Concatenate all features\n",
    "        if len(embeddings) > 1:\n",
    "            concatenated = Concatenate(name='feature_concat')(embeddings)\n",
    "        else:\n",
    "            concatenated = embeddings[0]\n",
    "        \n",
    "        # 4. Deep Neural Network layers\n",
    "        x = concatenated\n",
    "        \n",
    "        for i, hidden_size in enumerate(hidden_layers):\n",
    "            # Dense layer\n",
    "            x = Dense(\n",
    "                hidden_size,\n",
    "                activation='relu',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "                name=f'dense_{i+1}'\n",
    "            )(x)\n",
    "            \n",
    "            # Batch normalization\n",
    "            x = BatchNormalization(name=f'bn_{i+1}')(x)\n",
    "            \n",
    "            # Dropout for regularization\n",
    "            x = Dropout(dropout_rate, name=f'dropout_{i+1}')(x)\n",
    "        \n",
    "        # 5. Output layer for binary classification (CTR prediction)\n",
    "        output = Dense(1, activation='sigmoid', name='ctr_output')(x)\n",
    "        \n",
    "        # 6. Create and compile model\n",
    "        self.model = Model(inputs=inputs, outputs=output, name='CTR_DeepNN')\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def prepare_input_data(self, df, categorical_columns, numerical_columns, target_column):\n",
    "        \"\"\"\n",
    "        Prepare input data for the model\n",
    "        \n",
    "        Args:\n",
    "            df: pandas DataFrame\n",
    "            categorical_columns: list of categorical column names\n",
    "            numerical_columns: list of numerical column names  \n",
    "            target_column: name of target column\n",
    "        \"\"\"\n",
    "        X = {}\n",
    "        \n",
    "        # Categorical features (each as separate input)\n",
    "        for col in categorical_columns:\n",
    "            X[f'input_{col}'] = df[col].values.reshape(-1, 1)\n",
    "        \n",
    "        # Numerical features (combined as single input)\n",
    "        if numerical_columns:\n",
    "            X['numerical_features'] = df[numerical_columns].values\n",
    "        \n",
    "        y = df[target_column].values\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, \n",
    "              batch_size=512, epochs=100, verbose=1):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \"\"\"\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss' if X_val is not None else 'loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss' if X_val is not None else 'loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-7\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        validation_data = (X_val, y_val) if X_val is not None else None\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=validation_data,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        \"\"\"\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate the model\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        auc_score = roc_auc_score(y_test, y_pred)\n",
    "        logloss = log_loss(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Test AUC: {auc_score:.4f}\")\n",
    "        print(f\"Test LogLoss: {logloss:.4f}\")\n",
    "        \n",
    "        return auc_score, logloss\n",
    "\n",
    "# Example usage with your iPinYou dataset\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example of how to use the CTR model with your preprocessed data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example: Load your preprocessed iPinYou data\n",
    "    # df = pd.read_csv('your_preprocessed_ipinyou_data.csv')\n",
    "    \n",
    "    # Define your feature information based on your preprocessing\n",
    "    # Example feature configuration:\n",
    "    categorical_features_info = {\n",
    "        'weekday': 7,        # 7 days of week\n",
    "        'hour': 24,          # 24 hours\n",
    "        'useragent': 1000,   # vocab size after label encoding\n",
    "        'region': 500,       # vocab size after label encoding\n",
    "        'city': 1000,        # vocab size after label encoding\n",
    "        'adexchange': 10,    # number of ad exchanges\n",
    "        'domain': 5000,      # vocab size after label encoding\n",
    "        'slotid': 2000,      # vocab size after label encoding\n",
    "        'slotwidth': 50,     # number of unique slot widths\n",
    "        'slotheight': 50,    # number of unique slot heights\n",
    "        'slotvisibility': 5, # visibility categories\n",
    "        'slotformat': 10,    # format categories\n",
    "    }\n",
    "    \n",
    "    # Numerical features (from one-hot encoding or continuous features)\n",
    "    numerical_columns = ['bid_price', 'payprice']  # Add your numerical columns\n",
    "    numerical_features_count = len(numerical_columns)\n",
    "    \n",
    "    # Initialize the model\n",
    "    ctr_model = CTRDeepNN(\n",
    "        categorical_features_info=categorical_features_info,\n",
    "        numerical_features_count=numerical_features_count\n",
    "    )\n",
    "    \n",
    "    # Build the model architecture\n",
    "    model = ctr_model.build_model(\n",
    "        embedding_dim=50,\n",
    "        hidden_layers=[512, 256, 128, 64],  # 4 hidden layers\n",
    "        dropout_rate=0.3,\n",
    "        l2_reg=1e-5\n",
    "    )\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Example of preparing data (replace with your actual data)\n",
    "    \"\"\"\n",
    "    categorical_columns = list(categorical_features_info.keys())\n",
    "    target_column = 'click'  # your target column name\n",
    "    \n",
    "    X, y = ctr_model.prepare_input_data(\n",
    "        df, categorical_columns, numerical_columns, target_column\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Further split training data for validation\n",
    "    X_train_final = {}\n",
    "    X_val = {}\n",
    "    for key in X_train.keys():\n",
    "        X_train_final[key], X_val[key], y_train_final, y_val = train_test_split(\n",
    "            X_train[key], y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "        )\n",
    "    \n",
    "    # Train the model\n",
    "    history = ctr_model.train(\n",
    "        X_train_final, y_train_final,\n",
    "        X_val, y_val,\n",
    "        batch_size=512,\n",
    "        epochs=100\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    auc_score, logloss = ctr_model.evaluate(X_test, y_test)\n",
    "    \"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
